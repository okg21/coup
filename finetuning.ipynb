{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "import wandb\n",
    "\n",
    "import player\n",
    "import rl_player\n",
    "importlib.reload(player)\n",
    "importlib.reload(rl_player)\n",
    "from player import *\n",
    "from rl_player import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_GAMES = 100\n",
    "\n",
    "def run_simulation(players, shuffle_turn_order=False, num_games=NUM_GAMES):\n",
    "    winners = []\n",
    "\n",
    "    for game_index in range(num_games):\n",
    "        if shuffle_turn_order:\n",
    "            random.shuffle(players)\n",
    "        game = Game(players, debug=False)\n",
    "        \n",
    "        turn_count = 0\n",
    "        while len(game.game_state['players']) > 1:\n",
    "            game.simulate_turn()\n",
    "            turn_count += 1\n",
    "            if turn_count > 100:\n",
    "                break\n",
    "        winner = game.game_state['players'][0].name\n",
    "        winners.append(winner)\n",
    "    return winners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(agent, env, num_episodes, max_steps, batch_size, reward_dict, update_freq, sample_freq, sp_update_freq=10,\n",
    "                win_threshold=0.8, display_progress=False, save_model=False, player_types=[]):\n",
    "    total_rewards = []\n",
    "    epsilon = 1.0\n",
    "    win_rates = []\n",
    "    best_winrate = 0.0\n",
    "\n",
    "    for episode in tqdm(range(num_episodes), desc='Episode Loop'):\n",
    "        initial_state = env.reset()\n",
    "        state = initial_state\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        step = 0\n",
    "        \n",
    "        while not done and step <= max_steps:\n",
    "            # Environment stepping\n",
    "            action, next_state, reward, done = env.step(reward_dict)\n",
    "\n",
    "            # For adding experience\n",
    "            agent.add_experience(state, action, reward, next_state, done)\n",
    "\n",
    "            # Update the game state and history for the next iteration\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            step += 1\n",
    "        \n",
    "        # Replaying experience\n",
    "        if len(agent.replay_buffer) >= batch_size and step % update_freq == 0:\n",
    "            agent.replay_experience(batch_size, agent.name)\n",
    "\n",
    "        # Decay epsilon\n",
    "        agent.epsilon = max(agent.epsilon * agent.epsilon_decay, agent.epsilon_min)\n",
    "        \n",
    "        # Save the total reward for this episode\n",
    "        total_rewards.append(total_reward)\n",
    "\n",
    "        # Log the total reward for this episode\n",
    "        wandb.log({\"total_reward\": total_reward})\n",
    "\n",
    "        if display_progress and episode % sample_freq == 0:\n",
    "            winners = run_simulation(env.players)\n",
    "            win_pct = len([w for w in winners if w == agent.name]) / len(winners)\n",
    "            \n",
    "            # Log the win rate for this episode\n",
    "            wandb.log({\"win_rate\": win_pct})\n",
    "            win_rates.append(win_pct)\n",
    "            \n",
    "            # Print win rates\n",
    "            print(f\"Episode {episode + 1}/{num_episodes} - Win Rate: {win_pct} - Epsilon: {agent.epsilon}\")\n",
    "            \n",
    "            # If last sp_update_freq win rates are all over threshold, update the other players to main agent's weights\n",
    "            if len(win_rates) >= sp_update_freq and all([w > win_threshold for w in win_rates[-sp_update_freq:]]):\n",
    "                for player in env.players:\n",
    "                    if player.agent is not None and player.name != agent.name:\n",
    "                        # TO-DO: Maybe empty the replay buffer?\n",
    "                        player.agent.model.load_state_dict(agent.model.state_dict())\n",
    "                        player.agent.target_model.load_state_dict(agent.target_model.state_dict())\n",
    "                        print(f\"Updating {player.name} to {agent.name}'s weights\")\n",
    "\n",
    "    if display_progress:\n",
    "        # Plot the win rate over time\n",
    "        plt.plot([x * sample_freq + 1 for x in list(range(len(win_rates)))], win_rates, label=\"agent win rate\")\n",
    "        plt.plot([x * sample_freq + 1 for x in list(range(len(win_rates)))], [1 / len(env.players) for _ in range(len(win_rates))], label=\"expected win rate\", linestyle='dashed')\n",
    "        plt.legend()\n",
    "        plt.title('Q-Learning win rate over time')\n",
    "        plt.ylabel('win rate')\n",
    "        plt.xlabel('number of episodes')\n",
    "        plt.show()\n",
    "\n",
    "    if save_model:\n",
    "        agent.save_model('models/{0}-{1}-{2}'.format(len(env.players), num_episodes, ','.join(player_types)))\n",
    "        wandb.save('models/{0}-{1}-{2}'.format(len(env.players), num_episodes, ','.join(player_types)))\n",
    "\n",
    "    return total_rewards, win_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_play_training(n, reward_dict, learning_rate, gamma, epsilon_decay, h_dim, h_layers, tau,\n",
    "                        buffer_size, num_episodes, batch_size, max_steps_per_episode,\n",
    "                        update_freq, sample_freq, sp_update_freq, win_threshold, history_length=5, display_progress=True):\n",
    "  \n",
    "  action_dim = 5\n",
    "  block_size = 2\n",
    "  turn_dim = action_dim + n * 3 + block_size * 2\n",
    "  state_dim = (10 + 11 * n) + (history_length * turn_dim)\n",
    "  action_dim = 1 + 3 * n\n",
    "\n",
    "\n",
    "\n",
    "  agent1 = QLearningAgent(state_dim, action_dim, learning_rate, gamma, 'Player 1', True, history_length=history_length, \n",
    "                          epsilon_decay=epsilon_decay, h_dim= h_dim, h_layers=h_layers, tau=tau, buffer_size=buffer_size)\n",
    "  \n",
    "  agents = []\n",
    "  for i in range(2, n+1):\n",
    "    agent = QLearningAgent(state_dim, action_dim, learning_rate, gamma, f'Player {i}', False, history_length=history_length,\n",
    "                          epsilon_decay=epsilon_decay,h_dim= h_dim, h_layers=h_layers, tau=tau, buffer_size=buffer_size)\n",
    "    agents.append(agent)\n",
    "\n",
    "  RLTRAINING_FUNCS = {\n",
    "    'decision_fn': rltraining_decision, \n",
    "    'block_fn': income_block,\n",
    "    'dispose_fn': random_dispose,\n",
    "    'keep_fn': random_keep\n",
    "  }\n",
    "\n",
    "  players = [Player('Player 1', RLTRAINING_FUNCS, agent1)]\n",
    "  for i in range (2, n+1):\n",
    "    players.append(Player(f'Player {i}', RLTRAINING_FUNCS, agents[i-2]))\n",
    "  \n",
    "  env = Environment('Player 1', players)  \n",
    " \n",
    "\n",
    "  total_rewards, win_rates = train_agent(\n",
    "    agent1, env, num_episodes, max_steps_per_episode, batch_size, reward_dict, \n",
    "    update_freq=update_freq, sample_freq=sample_freq, sp_update_freq=sp_update_freq, win_threshold=win_threshold, \n",
    "    display_progress=display_progress\n",
    "  )\n",
    "\n",
    "  return total_rewards, win_rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"rl-coup-agent-tuning\")\n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'random',  # Can be \"grid\", \"random\", \"bayes\"\n",
    "    'metric': {\n",
    "      'name': 'win_rate',\n",
    "      'goal': 'maximize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'learning_rate': {\n",
    "            'min': 1e-4,\n",
    "            'max': 1e-2\n",
    "        },\n",
    "        'update_freq': {\n",
    "            'values': [1, 16, 32, 64, 128]\n",
    "        },\n",
    "        'gamma': {\n",
    "            'values': [0.95, 0.99, 0.999]\n",
    "        },\n",
    "        'epsilon_decay': {\n",
    "            'min': 0.99,\n",
    "            'max': 0.9999\n",
    "        },\n",
    "        'h_dim': {\n",
    "            'values': [128, 256, 512, 1024]\n",
    "        },\n",
    "        'h_layers': {\n",
    "            'values': [2, 3, 4, 5, 6]\n",
    "        },\n",
    "        'tau': {\n",
    "            'min': 1e-4,\n",
    "            'max': 1e-2\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [32, 64, 128, 1024, 4096]\n",
    "        },\n",
    "        'history_length': {\n",
    "            'values': [10, 20, 30, 40, 50]\n",
    "        },\n",
    "        'buffer_size': {\n",
    "            'values': [1e4, 1e5, 1e6, 1e7, 1e8]\n",
    "        },\n",
    "\n",
    "        'COIN_VALUE': {\n",
    "            'min': 0.05,\n",
    "            'max': 0.4\n",
    "        },\n",
    "        'CARD_VALUE': {\n",
    "            'min': 0.3,\n",
    "            'max': 2.0\n",
    "        },\n",
    "        'CARD_DIVERSITY_VALUE': {\n",
    "            'min': 0,\n",
    "            'max': 0.5\n",
    "        },\n",
    "        'WIN_VALUE': {\n",
    "            'min': 0.5,\n",
    "            'max': 5.0\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init() as run:\n",
    "        config = wandb.config\n",
    "        \n",
    "        reward_dict = {\n",
    "            'COIN_VALUE': config.COIN_VALUE,\n",
    "            'CARD_VALUE': config.CARD_VALUE,\n",
    "            'CARD_DIVERSITY_VALUE': config.CARD_DIVERSITY_VALUE,\n",
    "            'WIN_VALUE': config.WIN_VALUE\n",
    "        }\n",
    "        \n",
    "        # Call your training function with wandb's config\n",
    "        total_rewards, win_rates = self_play_training(\n",
    "            n=3,\n",
    "            reward_dict=reward_dict,\n",
    "            learning_rate=config.learning_rate,\n",
    "            gamma=config.gamma,\n",
    "            epsilon_decay=config.epsilon_decay,\n",
    "            h_dim=config.h_dim,\n",
    "            h_layers=config.h_layers,\n",
    "            tau=config.tau,\n",
    "            buffer_size=config.buffer_size,\n",
    "            num_episodes=10000,\n",
    "            batch_size=config.batch_size,\n",
    "            max_steps_per_episode=100,\n",
    "            update_freq=config.update_freq,\n",
    "            sp_update_freq=5,\n",
    "            sample_freq=1000,\n",
    "            win_threshold=0.9,\n",
    "            history_length=config.history_length,\n",
    "            display_progress=True\n",
    "        )\n",
    "\n",
    "        # Log maximum and average values of total_rewards\n",
    "        wandb.log({\"max_total_reward\": max(total_rewards)})\n",
    "        wandb.log({\"avg_total_reward\": sum(total_rewards) / len(total_rewards)})\n",
    "\n",
    "        # Log maximum and average values of win_rates\n",
    "        wandb.log({\"max_win_rate\": max(win_rates)})\n",
    "        wandb.log({\"avg_win_rate\": sum(win_rates) / len(win_rates)})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"rl-coup-agent-tuning\")\n",
    "wandb.agent(sweep_id, train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
